# CBM (Concept Bottleneck Models) — Essay-focused Demo

This repository implements Concept Bottleneck Models (CBM) for NLP with a focus on the Essay dataset. It includes training/evaluation pipelines, a FastAPI backend for inference, and a Streamlit frontend demo.

## Table of Contents

- Overview
- Pipelines
- Essay Dataset & Preparation
- Data Loading (as implemented)
- Learning Rate (two-step workflow)
- Run & Output (main.py)
- File Structure

## Overview

This project centers on the Essay dataset and provides two training pipelines:

- Standard: task-only fine-tuning without concept supervision
- Joint: joint training with concept and task supervision

LLM Mix Joint pipeline and other datasets (CEBaB, IMDB) are intentionally omitted from this README.

## Pipelines

- get_cbm_standard: Baseline fine-tuning without concept supervision
- get_cbm_joint: Joint training with concept and task supervision

Both pipelines are used by `cbm/main.py` to run experiments on the Essay dataset.

## Essay Dataset & Preparation

- Location: `dataset/essay/cleaned/{train,dev,test}.csv`
- Splits: validation split name maps `val` → file `dev` (per `cbm/data/essay.py`).
- Required columns (all rows):
  - `text`: string
  - `label`: integer in [0, 1, 2, 3, 4, 5] (6-class scoring)
  - Concept columns (8): `TC, UE, OC, GM, VA, SV, CTD, FR` (integers; used directly)

## Data Loading (as implemented)

The pipelines construct PyTorch `DataLoader`s internally. For custom usage with the Essay dataset (no variant parameter):

```python
from cbm.data.essay import EssayDataset
from cbm.models.loaders import load_model_and_tokenizer

model, tokenizer, hidden_size = load_model_and_tokenizer("bert-base-uncased")
train_ds = EssayDataset("train", tokenizer, max_len=512)
val_ds   = EssayDataset("val", tokenizer, max_len=512)
test_ds  = EssayDataset("test", tokenizer, max_len=512)
```

Supported backbones (via `cbm/models/loaders.py`): BERT (`bert-base-uncased`), RoBERTa (`roberta-base`), GPT‑2 (`gpt2`), BiLSTM (`lstm`).

## Learning Rate (two-step workflow)

1) Generate learning rates for Essay

```bash
cd cbm
python get_learning_rate.py --dataset essay
```

This produces `cbm/lr_rate/essay_lr_rate.csv` with best learning rates per model.

2) Auto-load learning rates during experiments

`cbm/main.py` loads learning rates via `cbm.utils.lr_loader.load_learning_rates(dataset)`.
If a CSV is missing or a model has no entry, a warning is printed and a built-in default LR is used.

## Run & Output (main.py)

Run experiments for Essay using both pipelines (Standard as “PLMs”, Joint as “CBE-PLMs”):

```bash
python cbm/main.py
```

Output: `cbm/result_essay.csv`

Columns:
- `dataset`: dataset name (essay)
- `data_type`: split label used for table grouping (`D` for Essay)
- `function`: pipeline alias (`PLMs` for Standard, `CBE-PLMs` for Joint)
- `model`: backbone model name (e.g., `bert-base-uncased`, `roberta-base`)
- `score`: list of `(accuracy, macro_f1)` tuples for the task
- `concept_score`: list of `(accuracy, macro_f1)` tuples for concepts (Joint only)

Example rows:

```csv
dataset,data_type,function,model,score,concept_score
essay,D,PLMs,bert-base-uncased,"[(0.85, 0.82)]","[]"
essay,D,CBE-PLMs,bert-base-uncased,"[(0.87, 0.84)]","[(0.78, 0.75)]"
```

## File Structure

High-level structure (only relevant parts shown):

```
cbm/
├── config/
│   └── defaults.py
├── data/
│   └── essay.py
├── models/
│   └── loaders.py
├── pipelines/
│   ├── standard.py
│   └── joint.py
├── training/
│   └── loops.py
├── utils/
│   └── lr_loader.py
├── get_learning_rate.py
├── lr_rate/
│   └── essay_lr_rate.csv          # generated by the LR step
├── main.py                        # runs Standard/Joint and writes result_essay.csv
└── result_essay.csv               # output of cbm/main.py

backend/
├── main.py                        # FastAPI inference service
└── ...

frontend/
├── app.py                         # Streamlit UI
└── ...

dataset/
└── essay/
    └── cleaned/
        ├── train.csv
        ├── dev.csv
        └── test.csv

saved_models/
└── <dataset>/
    └── <model_name>/
        ├── <model_name>_model_standard.pth
        ├── <model_name>_classifier_standard.pth
        ├── <model_name>_joint.pth
        └── <model_name>_ModelXtoCtoY_layer_joint.pth
```

Notes:
- Models are now organized by dataset under `saved_models/<dataset>/<model_name>/`.
- Set environment variable `CBM_DATASET` (default: `essay`) to control which dataset folder the backend uses when loading models.
- The backend will also auto-discover the correct dataset subfolder if `CBM_DATASET` is unset or the specified folder does not exist.
- Only the Essay dataset is documented here; other datasets are intentionally omitted.
- The LLM Mix Joint pipeline is not included in this README.


